{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP(Working).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rizGbCYIwDj_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en_core_web_lg\n",
        "! python -m spacy download en_vectors_web_lg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4fU_Jx4vyvBa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "# nlp = en_core_web_sm.load()\n",
        "# nlp = en_core_web_md.load()\n",
        "\n",
        "# nlp = spacy.load('en_core_web_md')\n",
        "nlp = spacy.load('en_core_web_lg')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VC2m1SzywNuB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Final_@3.csv\")\n",
        "# df  = pd.read_excel('Final_dataset@.xlsx')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G40JjvodBSga",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Drop column**"
      ]
    },
    {
      "metadata": {
        "id": "kZfKc13Iwbac",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drop unnamed column -----\n",
        "\n",
        "df.columns\n",
        "df=df.drop([ 'Unnamed: 0', 'Unnamed: 0.1' ],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8e-qyDlAaot1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aj8NbvuuMOL4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Arrange_Column**"
      ]
    },
    {
      "metadata": {
        "id": "LPTzNT42wh7B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cols = [['source','author','title','content','url','urlToImage','publishedAt','description','sentiment']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_akfZ3am9oFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KA9aqEd67n_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #create column in dataframe\n",
        "\n",
        "# raw_data = {'name': ['Willard Morris', 'Al Jennings', 'Omar Mullins', 'Spencer McDaniel'],\n",
        "#                       'age': [20, 19, 22, 21],\n",
        "#                       'favorite_color': ['blue', 'blue', 'yellow', \"green\"],\n",
        "#                       'test_one': [88, 92, 95, 70],\n",
        "#                       'test_two': [78, 100, 90, 85]}\n",
        "\n",
        "# df = pd.DataFrame(raw_data)\n",
        "# df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vBA5wyLQ9_-_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### add column into df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TbfSWy5D9_55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with open ('new.csv','a+',newline='') as f:\n",
        "#   obj = csv.writer(f)\n",
        "#   obj.writerow(['NAME','MOBILE'])\n",
        "#   name = input('enter the name')\n",
        "#   mob = int(input('enter the mobile number'))\n",
        "#   obj.writerow([name,mob])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iVmSSCcmXmeJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "date_index = set()\n",
        "date = []\n",
        "s1 = set()\n",
        "for i in range(len(df)):\n",
        "\n",
        "    # Process whole documents\n",
        "    text_ = str(df.iloc[0][3])\n",
        "    text_ = text_.replace('[', '').replace(']', '')\n",
        "    text = text_\n",
        "    doc = nlp(text)\n",
        "    for entity in doc.ents:\n",
        "      if entity.label_ == 'GPE':\n",
        "        date_index.add(i)\n",
        "        s1.add(entity.text)\n",
        "      elif entity.label_== None:\n",
        "        s1.add(entity.text)\n",
        "      else:\n",
        "        s1.add('NaN')\n",
        "    if len(s1)>0:\n",
        "      date.append(s1)\n",
        "    else:\n",
        "      date.append('NaN')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KSJjbWLdxcea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from description exract the NER's\n",
        "s=[]\n",
        "match = set()\n",
        "\n",
        "#description = [''' India vs New Zeland ODI Series The first book that gave me any real sense of the value of history was Swinton's \"World History,\" which I received on my thirteenth birthday.''' ]\n",
        "\n",
        "description = [''' South Korean tech firms Netmarble and Kakao as well as U.S. private equity firms Blackstone and Bain\n",
        "Capital submitted initial bids for control of gaming firm Nexon, the Korea Economic Daily said.\n",
        "Nexon is South Koreaâ€™s biggest gaming firm. Netmarble, a mobile gaming firm backed by Chinese tech giant Tencent, partnered with South Korean private equity fund MBK Partners for the bid, the report said on Thursday, citing investment banking sources.\n",
        "Netmarble and Kakao declined to comment, while Blackstone and Bain Capital were not immediately available for comment.''']\n",
        "\n",
        "for each_row in description:\n",
        "  doc = nlp(each_row)\n",
        "  displacy.render(nlp(each_row), jupyter=True, style='ent')\n",
        "  ents = [(e.text, e.label_) for e in doc.ents]\n",
        "  print(ents)\n",
        "  \n",
        "  \n",
        "#   for e in doc.ents:\n",
        "#     if e.label_ == 'EVENT':\n",
        "#       match .add(e.text)\n",
        "      \n",
        " \n",
        "  \n",
        "  \n",
        "#   for e in doc.ents:\n",
        "#     print(e,e.label_)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ej8RiUr87Y7R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# displacy.render(nlp(str(sentences)), jupyter=True, style='ent')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4IIwmnnCIEXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **FOR ALL ENTITES**"
      ]
    },
    {
      "metadata": {
        "id": "k20DbOuqxoV9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# gpe_index = set()\n",
        "# event_index = set()\n",
        "# date_index = set()\n",
        "# size_index = set()\n",
        "# product_index = set()\n",
        "# law_index = set()\n",
        "# person_index = set()\n",
        "# money_index = set()\n",
        "# org_index = set()\n",
        "\n",
        "# gpe = []\n",
        "# event_type = []\n",
        "# date = []\n",
        "# size = []\n",
        "# product = []\n",
        "# law = []\n",
        "# person = []\n",
        "# money = []\n",
        "# org = []\n",
        "\n",
        "# for i in range(len(df)):\n",
        "\n",
        "#     # Process whole documents\n",
        "#     text_ = str(df.iloc[i][2])\n",
        "   \n",
        "#     text_ = text_.replace('[', '').replace(']', '')\n",
        "#     text = text_\n",
        "#     doc = nlp(text)\n",
        "#     print('-----',doc)\n",
        "\n",
        "#     # Find named entities, phrases and concepts\n",
        "#     s1 = set()\n",
        "#     s2 = set()\n",
        "#     s3 = set()\n",
        "#     s4 = set()\n",
        "#     s5 = set()\n",
        "#     s6 = set()\n",
        "#     s7 = set()\n",
        "#     s8 = set()\n",
        "#     s9 = set()\n",
        "    \n",
        "#     for entity in doc.ents:\n",
        "#       if entity.label_ == 'GPE':\n",
        "#         date_index.add(i)\n",
        "#         s1.add(entity.text)\n",
        "#       elif entity.label_== None:\n",
        "#         s1.add(entity.text)\n",
        "#       else:\n",
        "#         s1.add('NaN')\n",
        "#     if len(s1)>0:\n",
        "#       gpe.append(s1)\n",
        "#     else:\n",
        "#       gpe.append('NaN')\n",
        "      \n",
        "#     for entity in doc.ents:\n",
        "#       if entity.label_ == 'EVENT':\n",
        "#         event_index.add(i)\n",
        "#         s2.add(entity.text)\n",
        "#       elif entity.label_==None:\n",
        "#         s2.add(entity.text)\n",
        "#       else:\n",
        "#         s2.add('NaN')\n",
        "#     if len(s2)>0:\n",
        "#       event_type.append(s2)\n",
        "#     else:\n",
        "#       event_type.append('NaN')\n",
        "    \n",
        "#     for entity in doc.ents:\n",
        "#       if entity.label_ == 'DATE':\n",
        "#         date_index.add(i)\n",
        "#         s3.add(entity.text)\n",
        "#       elif entity.label_== None:\n",
        "#         s3.add(entity.text)\n",
        "#       else:\n",
        "#         s3.add('NaN')\n",
        "#     if len(s3)>0:\n",
        "#       date.append(s3)\n",
        "#     else:\n",
        "#       date.append('NaN')\n",
        "   \n",
        "        \n",
        "#     for entity in doc.ents:\n",
        "#       if entity.label_ == 'QUANTITY':\n",
        "#         size_index.add(i)\n",
        "#         s4.add(entity.text)\n",
        "#       elif entity.label_==None:\n",
        "#         s4.add(entity.text)\n",
        "#       else:\n",
        "#         s4.add('NaN')\n",
        "#     if len(s4)>0:\n",
        "#       size.append(s4)\n",
        "#     else:\n",
        "#       size.append('NaN')\n",
        "      \n",
        "#     for entity in doc.ents:\n",
        "#       if entity.label_ == 'PRODUCT':\n",
        "#         product_index.add(i)\n",
        "#         s5.add(entity.text)\n",
        "\n",
        "#       elif entity.label_==None:\n",
        "#         s5.add(entity.text)\n",
        "#       else:\n",
        "#         s5.add('NaN')\n",
        "#     if len(s5)>0:\n",
        "#       product.append(s5)\n",
        "#     else:\n",
        "#       product.append('NaN')\n",
        "      \n",
        "#     for entity in doc.ents:\n",
        "#       if entity.label_ == 'LAW':\n",
        "#         law_index.add(i)\n",
        "#         s6.add(entity.text)\n",
        "#       elif entity.label_==None:\n",
        "#         s6.add(entity.text)\n",
        "#       else:\n",
        "#         s6.add('NaN')\n",
        "\n",
        "#     if len(s6)>0:\n",
        "#       law.append(s6)\n",
        "#     else:\n",
        "#       law.append('NaN')\n",
        "        \n",
        "#     for entity in doc.ents:\n",
        "#       if entity.label_ == 'PERSON':\n",
        "        \n",
        "#         person_index.add(i)\n",
        "#         s7.add(entity.text)\n",
        "\n",
        "#       elif entity.label_==None:\n",
        "#         s7.add(entity.text)\n",
        "#       else:\n",
        "#         s7.add('NaN')\n",
        "  \n",
        "#     if len(s7)>0:\n",
        "#       person.append(s7)\n",
        "#     else:\n",
        "#       person.append('NaN')\n",
        "      \n",
        "#     for entity in doc.ents:\n",
        "#       if entity.label_ == 'MONEY':\n",
        "#         money_index.add(i)\n",
        "#         s8.add(entity.text)\n",
        "\n",
        "#       elif entity.label_==None:\n",
        "#         s8.add(entity.text)\n",
        "#       else:\n",
        "#         s8.add('NaN')\n",
        "        \n",
        "#     if len(s8)>0:\n",
        "#       money.append(s8)\n",
        "#     else:\n",
        "#       money.append('NaN')\n",
        "      \n",
        "#     for entity in doc.ents:\n",
        "#       if entity.label_ == 'ORG':\n",
        "#         s9.add(entity.text)\n",
        "#       elif entity.label_== None:\n",
        "#         s9.add(entity.text)\n",
        "#       else:\n",
        "#         s9.add('NaN')\n",
        "#     if len(s9)>0:\n",
        "#       org.append(s9)\n",
        "#     else:\n",
        "#       org.append('NaN')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9fIJWeIISW2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **FOR Specific**"
      ]
    },
    {
      "metadata": {
        "id": "zCkBFSeQ6CGX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gpe_index = set()\n",
        "event_index = set()\n",
        "date_index = set()\n",
        "product_index = set()\n",
        "money_index = set()\n",
        "org_index = set()\n",
        "\n",
        "gpe = []\n",
        "event_type = []\n",
        "date = []\n",
        "product = []\n",
        "money = []\n",
        "org = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "\n",
        "    # Process whole documents\n",
        "    text_ = str(df.iloc[i][2])\n",
        "   \n",
        "    text_ = text_.replace('[', '').replace(']', '')\n",
        "    text = text_\n",
        "    doc = nlp(text)\n",
        "    print('-----',doc)\n",
        "\n",
        "    # Find named entities, phrases and concepts\n",
        "    s1 = set()\n",
        "    s2 = set()\n",
        "    s3 = set()\n",
        "    s4 = set()\n",
        "    s5 = set()\n",
        "    s6 = set()\n",
        "    \n",
        "    for entity in doc.ents:\n",
        "      if entity.label_ == 'GPE':\n",
        "        date_index.add(i)\n",
        "        s1.add(entity.text)\n",
        "      elif entity.label_== None:\n",
        "        s1.add(entity.text)\n",
        "      else:\n",
        "        s1.add('NaN')\n",
        "    if len(s1)>0:\n",
        "      gpe.append(s1)\n",
        "    else:\n",
        "      gpe.append('NaN')\n",
        "      \n",
        "    for entity in doc.ents:\n",
        "      if entity.label_ == 'EVENT':\n",
        "        event_index.add(i)\n",
        "        s2.add(entity.text)\n",
        "      elif entity.label_==None:\n",
        "        s2.add(entity.text)\n",
        "      else:\n",
        "        s2.add('NaN')\n",
        "    if len(s2)>0:\n",
        "      event_type.append(s2)\n",
        "    else:\n",
        "      event_type.append('NaN')\n",
        "    \n",
        "    for entity in doc.ents:\n",
        "      if entity.label_ == 'DATE':\n",
        "        date_index.add(i)\n",
        "        s3.add(entity.text)\n",
        "      elif entity.label_== None:\n",
        "        s3.add(entity.text)\n",
        "      else:\n",
        "        s3.add('NaN')\n",
        "    if len(s3)>0:\n",
        "      date.append(s3)\n",
        "    else:\n",
        "      date.append('NaN')\n",
        "   \n",
        "    for entity in doc.ents:\n",
        "      if entity.label_ == 'PRODUCT':\n",
        "        product_index.add(i)\n",
        "        s4.add(entity.text)\n",
        "\n",
        "      elif entity.label_==None:\n",
        "        s4.add(entity.text)\n",
        "      else:\n",
        "        s4.add('NaN')\n",
        "    if len(s4)>0:\n",
        "      product.append(s4)\n",
        "    else:\n",
        "      product.append('NaN')\n",
        "      \n",
        "      \n",
        "    for entity in doc.ents:\n",
        "      if entity.label_ == 'MONEY':\n",
        "        money_index.add(i)\n",
        "        s5.add(entity.text)\n",
        "\n",
        "      elif entity.label_==None:\n",
        "        s5.add(entity.text)\n",
        "      else:\n",
        "        s5.add('NaN')\n",
        "        \n",
        "    if len(s5)>0:\n",
        "      money.append(s5)\n",
        "    else:\n",
        "      money.append('NaN')\n",
        "      \n",
        "    for entity in doc.ents:\n",
        "      if entity.label_ == 'ORG':\n",
        "        s6.add(entity.text)\n",
        "      elif entity.label_== None:\n",
        "        s6.add(entity.text)\n",
        "      else:\n",
        "        s6.add('NaN')\n",
        "    if len(s6)>0:\n",
        "      org.append(s6)\n",
        "    else:\n",
        "      org.append('NaN')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APhBy8HEH192",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# len(size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Lvaj2TG6NZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df.drop(columns=['Orgnazation']) #from column from dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e5mSDI9N69lf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "org"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rDKXB1hr2b88",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for l in org:\n",
        "    print('---',l)\n",
        "    for w in l:\n",
        "      print(w)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tc9ID_FACk7a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RLIwcfw3Rfem",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# df['organization'] = df['organization'].str.strip('{').strip('}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZL1-9dKozE5r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "gpe = []\n",
        "event_type = []\n",
        "date = []\n",
        "size = []\n",
        "product = []\n",
        "law = []\n",
        "person = []\n",
        "money = []\n",
        "org = []\n",
        "# Process whole documents\n",
        "\n",
        "text = input('enter Text ')\n",
        "doc = nlp(text)\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "s1 = set()\n",
        "s2 = set()\n",
        "s3 = set()\n",
        "s4 = set()\n",
        "s5 = set()\n",
        "s6 = set()\n",
        "s7 = set()\n",
        "s8 = set()\n",
        "s9 = set()\n",
        "\n",
        "for entity in doc.ents:\n",
        "  if entity.label_ == 'ORG':\n",
        "    s9.add(entity.text)\n",
        "  elif entity.label_== None:\n",
        "    s9.add(entity.text)\n",
        "  else:\n",
        "    s9.add('NaN')\n",
        "if len(s9)>0:\n",
        "  org.append(s9)\n",
        "else:\n",
        "  org.append('NaN')\n",
        "\n",
        "\n",
        "for entity in doc.ents:\n",
        "  if entity.label_ == 'GPE':\n",
        "    s1.add(entity.text)\n",
        "  elif entity.label_== None:\n",
        "    s1.add(entity.text)\n",
        "  else:\n",
        "    s1.add('NaN')\n",
        "if len(s1)>0:\n",
        "  gpe.append(s1)\n",
        "else:\n",
        "  gpe.append('NaN')\n",
        "\n",
        "for entity in doc.ents:\n",
        "  if entity.label_ == 'EVENT':\n",
        "    s2.add(entity.text)\n",
        "  elif entity.label_==None:\n",
        "    s2.add(entity.text)\n",
        "  else:\n",
        "    s2.add('NaN')\n",
        "if len(s2)>0:\n",
        "  event_type.append(s2)\n",
        "else:\n",
        "  event_type.append('NaN')\n",
        "\n",
        "for entity in doc.ents:\n",
        "  if entity.label_ == 'DATE':\n",
        "    s3.add(entity.text)\n",
        "  elif entity.label_== None:\n",
        "    s3.add(entity.text)\n",
        "  else:\n",
        "    s3.add('NaN')\n",
        "if len(s3)>0:\n",
        "  date.append(s3)\n",
        "else:\n",
        "  date.append('NaN')\n",
        "\n",
        "\n",
        "for entity in doc.ents:\n",
        "  if entity.label_ == 'QUANTITY':\n",
        "    s4.add(entity.text)\n",
        "  elif entity.label_==None:\n",
        "    s4.add(entity.text)\n",
        "  else:\n",
        "    s4.add('NaN')\n",
        "if len(s4)>0:\n",
        "  size.append(s4)\n",
        "else:\n",
        "  size.append('NaN')\n",
        "\n",
        "for entity in doc.ents:\n",
        "  if entity.label_ == 'PRODUCT':\n",
        "    s5.add(entity.text)\n",
        "\n",
        "  elif entity.label_==None:\n",
        "    s5.add(entity.text)\n",
        "  else:\n",
        "    s5.add('NaN')\n",
        "if len(s5)>0:\n",
        "  product.append(s5)\n",
        "else:\n",
        "  product.append('NaN')\n",
        "\n",
        "for entity in doc.ents:\n",
        "  if entity.label_ == 'LAW':\n",
        "    s6.add(entity.text)\n",
        "  elif entity.label_==None:\n",
        "    s6.add(entity.text)\n",
        "  else:\n",
        "    s6.add('NaN')\n",
        "\n",
        "if len(s6)>0:\n",
        "  law.append(s6)\n",
        "else:\n",
        "  law.append('NaN')\n",
        "\n",
        "for entity in doc.ents:\n",
        "  if entity.label_ == 'PERSON':\n",
        "    s7.add(entity.text)\n",
        "\n",
        "  elif entity.label_==None:\n",
        "    s7.add(entity.text)\n",
        "  else:\n",
        "    s7.add('NaN')\n",
        "\n",
        "if len(s7)>0:\n",
        "  person.append(s7)\n",
        "else:\n",
        "  person.append('NaN')\n",
        "\n",
        "for entity in doc.ents:\n",
        "  if entity.label_ == 'MONEY':\n",
        "    s8.add(entity.text)\n",
        "\n",
        "  elif entity.label_==None:\n",
        "    s8.add(entity.text)\n",
        "  else:\n",
        "    s8.add('NaN')\n",
        "\n",
        "if len(s8)>0:\n",
        "  money.append(s8)\n",
        "else:\n",
        "  money.append('NaN')\n",
        "\n",
        "  \n",
        "print('Organization',org)\n",
        "print('Geographic Region is',gpe)\n",
        "print('Event_type is ',event_type)\n",
        "print('Date is ',date)\n",
        "print('Size is',size)\n",
        "print('Product ',product)\n",
        "print('Law is ',law)\n",
        "print('Person is',person)\n",
        "print('Money is',money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LBhIG5bsJRGb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **TO_Add_Column_into_df**"
      ]
    },
    {
      "metadata": {
        "id": "ofpROPiCqTAk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['organization'] = org\n",
        "df['loss_date'] = date\n",
        "df['geographical_region'] = gpe\n",
        "df['credit_related'] = None\n",
        "df['event_Type'] = event_type\n",
        "df['product'] = product\n",
        "df['process'] = None\n",
        "df['loss_money'] = money\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e2kvA4CypLsN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S-Hwy4-WoWAO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Save csv**"
      ]
    },
    {
      "metadata": {
        "id": "EIHRHbQRMkC2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv('final_data_43.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IrT4fYKRzqz9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# B=[]\n",
        "# for i in df_new.index:\n",
        "#   if i in law_index:\n",
        "#     B.append(df_new.iloc[i])\n",
        "\n",
        "# # df_new1=pd.DataFrame(B)\n",
        "# # df_new1['Law']=law\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7j5CdpKO6PVZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ny-cNejCyHas",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Emoji Sentiment Analysis**"
      ]
    },
    {
      "metadata": {
        "id": "OeCe2WlHJjvl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = English()  # we only want the tokenizer, so no need to load a model\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pos_emoji = [u'ðŸ˜€', u'ðŸ˜ƒ', u'ðŸ˜‚', u'ðŸ¤£', u'ðŸ˜Š', u'ðŸ˜']  # positive emoji\n",
        "neg_emoji = [u'ðŸ˜ž', u'ðŸ˜ ', u'ðŸ˜©', u'ðŸ˜¢', u'ðŸ˜­', u'ðŸ˜’']  # negative emoji\n",
        "\n",
        "# add patterns to match one or more emoji tokens\n",
        "pos_patterns = [[{'ORTH': emoji}] for emoji in pos_emoji]\n",
        "neg_patterns = [[{'ORTH': emoji}] for emoji in neg_emoji]\n",
        "\n",
        "# function to label the sentiment\n",
        "def label_sentiment(matcher, doc, i, matches):\n",
        "    match_id, start, end = matches[i]\n",
        "    if doc.vocab.strings[match_id] == 'HAPPY':  # don't forget to get string!\n",
        "        doc.sentiment += 0.1  # add 0.1 for positive sentiment\n",
        "    elif doc.vocab.strings[match_id] == 'SAD':\n",
        "        doc.sentiment -= 0.1  # subtract 0.1 for negative sentiment\n",
        "\n",
        "matcher.add('HAPPY', label_sentiment, *pos_patterns)  # add positive pattern\n",
        "matcher.add('SAD', label_sentiment, *neg_patterns)  # add negative pattern\n",
        "\n",
        "# add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
        "matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
        "\n",
        "doc = nlp(u\"Hello world ðŸ˜ ðŸ˜ƒ ðŸ˜ž #MondayMotivation\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    string_id = doc.vocab.strings[match_id]  # look up string ID\n",
        "    span = doc[start:end]\n",
        "    print(string_id, span.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QyoSxNHa4oVj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ** Large Text convert into sentence using Spacy**"
      ]
    },
    {
      "metadata": {
        "id": "iq_xmyDoxqUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "text = '''Spacy does not comes with an easily usable function for sentiment analysis. TextBlob, however, is an excellent library to use for performing quick sentiment analysis.\n",
        "If you want to use exclusively Spacy, a good idea would be to tokenize the text and perform an LSTM sentiment \n",
        "classification after training a model with Keras.\n",
        "Alternatively,you could also check out this example in their official documentation - it might be helpful depending on your purpose.'''\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "  print(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sEDiA50G5fbD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "first_sent = next(doc.sents) # only for one sentence\n",
        "for word in first_sent:\n",
        "  print(word.lemma_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4d1UWKmW74mv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plac\n",
        "import random\n",
        "import pathlib\n",
        "import cytoolz\n",
        "import numpy\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.optimizers import Adam\n",
        "import thinc.extra.datasets\n",
        "from spacy.compat import pickle\n",
        "import spacy\n",
        "\n",
        "\n",
        "class SentimentAnalyser(object):\n",
        "    @classmethod\n",
        "    def load(cls, path, nlp, max_length=100):\n",
        "        with (path / 'config.json').open() as file_:\n",
        "            model = model_from_json(file_.read())\n",
        "        with (path / 'model').open('rb') as file_:\n",
        "            lstm_weights = pickle.load(file_)\n",
        "        embeddings = get_embeddings(nlp.vocab)\n",
        "        model.set_weights([embeddings] + lstm_weights)\n",
        "        return cls(model, max_length=max_length)\n",
        "\n",
        "    def __init__(self, model, max_length=100):\n",
        "        self._model = model\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        X = get_features([doc], self.max_length)\n",
        "        y = self._model.predict(X)\n",
        "        self.set_sentiment(doc, y)\n",
        "\n",
        "    def pipe(self, docs, batch_size=1000, n_threads=2):\n",
        "        for minibatch in cytoolz.partition_all(batch_size, docs):\n",
        "            minibatch = list(minibatch)\n",
        "            sentences = []\n",
        "            for doc in minibatch:\n",
        "                sentences.extend(doc.sents)\n",
        "            Xs = get_features(sentences, self.max_length)\n",
        "            ys = self._model.predict(Xs)\n",
        "            for sent, label in zip(sentences, ys):\n",
        "                sent.doc.sentiment += label - 0.5\n",
        "            for doc in minibatch:\n",
        "                yield doc\n",
        "\n",
        "    def set_sentiment(self, doc, y):\n",
        "        doc.sentiment = float(y[0])\n",
        "        # Sentiment has a native slot for a single float.\n",
        "        # For arbitrary data storage, there's:\n",
        "        # doc.user_data['my_data'] = y\n",
        "\n",
        "\n",
        "def get_labelled_sentences(docs, doc_labels):\n",
        "    labels = []\n",
        "    sentences = []\n",
        "    for doc, y in zip(docs, doc_labels):\n",
        "        for sent in doc.sents:\n",
        "            sentences.append(sent)\n",
        "            labels.append(y)\n",
        "    return sentences, numpy.asarray(labels, dtype='int32')\n",
        "\n",
        "\n",
        "def get_features(docs, max_length):\n",
        "    docs = list(docs)\n",
        "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
        "    for i, doc in enumerate(docs):\n",
        "        j = 0\n",
        "        for token in doc:\n",
        "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
        "            if vector_id >= 0:\n",
        "                Xs[i, j] = vector_id\n",
        "            else:\n",
        "                Xs[i, j] = 0\n",
        "            j += 1\n",
        "            if j >= max_length:\n",
        "                break\n",
        "    return Xs\n",
        "\n",
        "\n",
        "def train(train_texts, train_labels, dev_texts, dev_labels,\n",
        "          lstm_shape, lstm_settings, lstm_optimizer, batch_size=100,\n",
        "          nb_epoch=5, by_sentence=True):\n",
        "    \n",
        "    print(\"Loading spaCy\")\n",
        "    nlp = spacy.load('en_vectors_web_lg')\n",
        "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "    embeddings = get_embeddings(nlp.vocab)\n",
        "    model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
        "    \n",
        "    print(\"Parsing texts...\")\n",
        "    train_docs = list(nlp.pipe(train_texts))\n",
        "    dev_docs = list(nlp.pipe(dev_texts))\n",
        "    if by_sentence:\n",
        "        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
        "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
        "\n",
        "    train_X = get_features(train_docs, lstm_shape['max_length'])\n",
        "    dev_X = get_features(dev_docs, lstm_shape['max_length'])\n",
        "    model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels),\n",
        "              epochs=nb_epoch, batch_size=batch_size)\n",
        "    return model\n",
        "\n",
        "\n",
        "def compile_lstm(embeddings, shape, settings):\n",
        "    model = Sequential()\n",
        "    model.add(\n",
        "        Embedding(\n",
        "            embeddings.shape[0],\n",
        "            embeddings.shape[1],\n",
        "            input_length=shape['max_length'],\n",
        "            trainable=False,\n",
        "            weights=[embeddings],\n",
        "            mask_zero=True\n",
        "        )\n",
        "    )\n",
        "    model.add(TimeDistributed(Dense(shape['nr_hidden'], use_bias=False)))\n",
        "    model.add(Bidirectional(LSTM(shape['nr_hidden'],\n",
        "                                 recurrent_dropout=settings['dropout'],\n",
        "                                 dropout=settings['dropout'])))\n",
        "    model.add(Dense(shape['nr_class'], activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(lr=settings['lr']), loss='binary_crossentropy',\n",
        "\t\t  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_embeddings(vocab):\n",
        "    return vocab.vectors.data\n",
        "\n",
        "\n",
        "def evaluate(model_dir, texts, labels, max_length=100):\n",
        "    nlp = spacy.load('en_vectors_web_lg')\n",
        "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "    nlp.add_pipe(SentimentAnalyser.load(model_dir, nlp, max_length=max_length))\n",
        "\n",
        "    correct = 0\n",
        "    i = 0\n",
        "    for doc in nlp.pipe(texts, batch_size=1000, n_threads=4):\n",
        "        correct += bool(doc.sentiment >= 0.5) == bool(labels[i])\n",
        "        i += 1\n",
        "    return float(correct) / i\n",
        "\n",
        "\n",
        "def read_data(data_dir, limit=0):\n",
        "    examples = []\n",
        "    for subdir, label in (('pos', 1), ('neg', 0)):\n",
        "        for filename in (data_dir / subdir).iterdir():\n",
        "            with filename.open() as file_:\n",
        "                text = file_.read()\n",
        "            examples.append((text, label))\n",
        "    random.shuffle(examples)\n",
        "    if limit >= 1:\n",
        "        examples = examples[:limit]\n",
        "    return zip(*examples) # Unzips into two lists\n",
        "\n",
        "\n",
        "@plac.annotations(\n",
        "    train_dir=(\"Location of training file or directory\"),\n",
        "    dev_dir=(\"Location of development file or directory\"),\n",
        "    model_dir=(\"Location of output model directory\",),\n",
        "    is_runtime=(\"Demonstrate run-time usage\", \"flag\", \"r\", bool),\n",
        "    nr_hidden=(\"Number of hidden units\", \"option\", \"H\", int),\n",
        "    max_length=(\"Maximum sentence length\", \"option\", \"L\", int),\n",
        "    dropout=(\"Dropout\", \"option\", \"d\", float),\n",
        "    learn_rate=(\"Learn rate\", \"option\", \"e\", float),\n",
        "    nb_epoch=(\"Number of training epochs\", \"option\", \"i\", int),\n",
        "    batch_size=(\"Size of minibatches for training LSTM\", \"option\", \"b\", int),\n",
        "    nr_examples=(\"Limit to N examples\", \"option\", \"n\", int)\n",
        ")\n",
        "def main(model_dir=None, train_dir=None, dev_dir=None,\n",
        "         is_runtime=False,\n",
        "         nr_hidden=64, max_length=100, # Shape\n",
        "         dropout=0.5, learn_rate=0.001, # General NN config\n",
        "         nb_epoch=5, batch_size=256, nr_examples=-1):  # Training params\n",
        "    if model_dir is not None:\n",
        "        model_dir = pathlib.Path(model_dir)\n",
        "    if train_dir is None or dev_dir is None:\n",
        "        imdb_data = thinc.extra.datasets.imdb()\n",
        "    if is_runtime:\n",
        "        if dev_dir is None:\n",
        "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
        "        else:\n",
        "            dev_texts, dev_labels = read_data(dev_dir)\n",
        "        acc = evaluate(model_dir, dev_texts, dev_labels, max_length=max_length)\n",
        "        print(acc)\n",
        "    else:\n",
        "        if train_dir is None:\n",
        "            train_texts, train_labels = zip(*imdb_data[0])\n",
        "        else:\n",
        "            print(\"Read data\")\n",
        "            train_texts, train_labels = read_data(train_dir, limit=nr_examples)\n",
        "        if dev_dir is None:\n",
        "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
        "        else:\n",
        "            dev_texts, dev_labels = read_data(dev_dir, imdb_data, limit=nr_examples)\n",
        "        train_labels = numpy.asarray(train_labels, dtype='int32')\n",
        "        dev_labels = numpy.asarray(dev_labels, dtype='int32')\n",
        "        lstm = train(train_texts, train_labels, dev_texts, dev_labels,\n",
        "                     {'nr_hidden': nr_hidden, 'max_length': max_length, 'nr_class': 1},\n",
        "                     {'dropout': dropout, 'lr': learn_rate},\n",
        "                     {},\n",
        "                     nb_epoch=nb_epoch, batch_size=batch_size)\n",
        "        weights = lstm.get_weights()\n",
        "        if model_dir is not None:\n",
        "            with (model_dir / 'model').open('wb') as file_:\n",
        "                pickle.dump(weights[1:], file_)\n",
        "            with (model_dir / 'config.json').open('w') as file_:\n",
        "                file_.write(lstm.to_json())\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     plac.call(main)\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"-1\",\"--image\", required = True, help = \"Path to the image\")\n",
        "    args = vars(ap.parse_args())\n",
        "    print (args['image'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p0rWAuj8tx9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ib4HYlmrDpm2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **sentiment analysis**"
      ]
    },
    {
      "metadata": {
        "id": "pSospnd4D0Bg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LFCxIcrrEN-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('250000_dataset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S3NkmSRfLYBW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Av5jJuI_D3hY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "header= ['target','id','date','flag','user','text']\n",
        "data.set_axis(header,axis=1,inplace=True)\n",
        "data_ready = data.drop(['id','date','flag','user'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-dsJ3ZHtLRu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_ready.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16dHGCbfbHFH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, SpatialDropout1D, Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from nltk.stem import PorterStemmer\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I2LosyI2ADAN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJU-F3jCbPz7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "punct = list(string.punctuation)\n",
        "stopword_list = stopwords.words('english') + punct + ['rt','via', '...']\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hXDGzUFDuz1f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define a function for data cleaning / preprocessing\n",
        "def sentense_to_words(raw_review):\n",
        "    text = raw_review.lower()      \n",
        "    tokens = TweetTokenizer().tokenize(text=text)    \n",
        "    clean_tokens= [stemmer.stem(tok) for tok in tokens if tok not in stopword_list and not tok.isdigit() and not tok.startswith('@')and not tok.startswith('#')and not tok.startswith('http')] \n",
        "    return( \" \".join(clean_tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1VJv2G-ZvImv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent =sentense_to_words('@iamjazzyfizzle  I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIYoQVBNw0ok",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test the function for one tweet\n",
        "tweet = sentense_to_words( data_ready['text'][49002])\n",
        "print(data_ready['text'][49002], len(tweet) )\n",
        "print(tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEWAcHB6yKia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# processing all tweets\n",
        "corpus=[]\n",
        "sent_len_list=[]\n",
        "for i in range(0,len(data_ready)):\n",
        "    corp= sentense_to_words(data_ready['text'][i])\n",
        "    sent_len_list.append(len(corp))\n",
        "    corpus.append(corp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u0EWlJtA85pJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_len=50\n",
        "max_features=2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b8DvV-bT9IsG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# creating vectorized corpus and padding\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "X = tokenizer.texts_to_sequences(corpus)\n",
        "X = pad_sequences(X, maxlen=max_len)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lo5Uedxa9Oyh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# relabel the sentiments 4 as 1\n",
        "label = data_ready['target'].values\n",
        "new_label = list(map(lambda x:x if x!= 4 else 1,label))\n",
        "Y = to_categorical(new_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kw3xDkCy9W2w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wS48kdrI95xV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier= Sequential()\n",
        "classifier.add(Embedding(max_features,150,mask_zero=True))\n",
        "classifier.add(LSTM(100,dropout=0.3,recurrent_dropout=0.3,return_sequences=False))\n",
        "classifier.add(Dense(2, activation='softmax'))\n",
        "classifier.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "classifier.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dcDNdiiw9-wO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "callback = [EarlyStopping(monitor='val_loss', patience=2),ModelCheckpoint(filepath='best_model1.h5', monitor='val_loss', save_best_only=True)]\n",
        "classifier.fit(X_train, y_train,batch_size=100,epochs=12,callbacks=callback ,validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dynUclEL-om4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xmm45WlnVMh3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Summerasation using spacy"
      ]
    },
    {
      "metadata": {
        "id": "QRkwQukvrVlT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Text_Summerasation_link_find](https://hackernoon.com/summarization-with-wine-reviews-using-spacy-b49f18399577)"
      ]
    },
    {
      "metadata": {
        "id": "nawb-E9HVUXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "from time import time\n",
        "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "%matplotlib inline\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dy9Xp96uoE-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stopwords = stopwords.words('english')\n",
        "sns.set_context('notebook')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kG7bMjEvAw4n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qjnbeWctVbqy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reviews = pd.read_csv(\"../input/winemag-data-130k-v2.csv\", nrows=5000,usecols =['points', 'title', 'description'],encoding='latin1')\n",
        "reviews = pd.read_csv('Final_@3.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHSj7c1e5w8H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews['description']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2w1kt6j6f8ys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reviews['description'] = '''Renaissance Europe, the arrival of mechanical movable type printing introduced the era of \n",
        "# mass communication which permanently altered the structure of society.Renaissance Europe and introduced mass communication\n",
        "# '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCcNT7oIYHtH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **paragraph to sentence**"
      ]
    },
    {
      "metadata": {
        "id": "B6WGQf1jdyJU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clVsDvlrhMss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nnAlE4x4gYMr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8BP0tAGKiWOd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews['description'][5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MeS8S8-RQ3yQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reviews['description']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QiRkRSXXfUwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Clean text before feeding it to spaCy**"
      ]
    },
    {
      "metadata": {
        "id": "-KuZpmNfoxuu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "punctuation = string.punctuation\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "from  spacy.lang.en.stop_words import STOP_WORDS\n",
        "stopword = list(STOP_WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ffga6XoIoPhf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cleanup_text(docs):\n",
        "\tmytokens =  parser(docs)\n",
        "\tmytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\"  else word.lower_ for word in mytokens]\n",
        "\tmytokens = [word for word in mytokens if word not in stopword and word not in punctuation]\n",
        "\treturn mytokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U56kOR5Eohyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = cleanup_text('''Brutal cold that can cause frostbite within a few minutes blasted the Midwest for a second day and blew into the Northeast on Thursday, snarling air traffic and delaying Amazon and other package deliveries. Amazon said it closed some buildings, including fulfillment centers across the Midwest that were affected by the numbing low temperatures. \"We work hard to deliver on our fast, free shipping promise, but weather conditions are out of our control,\" Amazon said in a statement. \"Customer service is available to work with any customer who is experiencing an issue.\" United Parcel Service said it had suspended package deliveries and pickups in parts of upstate New York and across a wide swath of the Midwest for safety reasons as temperatures remained low and federal forecasters warned of \"dangerously low wind chill values.\" UPS listed affected areas by ZIP codes in the Midwest and New York on its website. Rival FedEx closed some of its offices early, including in Chicago and Detroit, scaled back service in cities across the Midwest, saying: \"Our priority is always safety and providing service to the best of our ability.\" The U.S. Postal Service suspended deliveries in some cities in the Midwest on Thursday \"to ensure the safety and well-being of our employees.\" The stinging cold hobbled Chicago\\'s main airports on Thursday, as it was too cold for ground workers to access and service many aircraft. Airlines canceled more than 1,400 flights on Thursday at Chicago\\'s O\\'Hare International Airport, more than half of the daily schedule at the United Airlines and American Airlines hub. More than 40 percent of the schedule, or 237 flights at Chicago Midway International Airport, were canceled, according to flight-tracking site FlightAware. Hundreds of other flights were canceled in Chicago on Wednesday due to the cold. tweet Temperatures at O\\'Hare at 5:30 a.m. were minus 21 degrees, with wind chills of 37 degrees below zero, the National Weather Service said. Delta Air Lines, American, United, Spirit Airlines and JetBlue Airways said they would waive date-change fees for travelers affected by the severe weather. Southwest Airlines, which doesn\\'t have date-change fees, said it wouldn\\'t charge travelers booked in and out of more than two dozen U.S. airports the fare difference to fly at a later date due to the extreme weather. Southwest canceled 410 flights on Thursday, about 10 percent of its schedule. While aircraft can take generally take off in low temperatures, the bitter cold limits how long ground workers can remain loading baggage or provide fuel and other essential services for aircraft. Delta, which operates hubs in Minneapolis and Detroit, prepared for the frigid conditions by increasing staffing of ground workers. Employees can take more frequent breaks indoors, as the airline can prepare for the low temperatures by moving some aircraft into heated hangars overnight, said spokesman Michael Thomas.   Playing Share this video...''')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFaie421e2P9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~Â©'\n",
        "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
        "def cleanup_text(docs, logging=False):\n",
        "    texts = []\n",
        "    doc = nlp(docs, disable=['parser', 'ner'])\n",
        "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
        "    \n",
        "    tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations] \n",
        "    tokens = ' '.join(tokens)\n",
        "    texts.append(tokens)\n",
        "    return pd.Series(texts)\n",
        "  \n",
        "\n",
        "reviews['Description_Cleaned'] = reviews['description'].apply(lambda x: cleanup_text(x, False))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W55rAxjlChfd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews['Description_Cleaned']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G2tK6fd-faks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Reviews description with punctuatin and stopwords---\\n')\n",
        "print(reviews['description'][5])\n",
        "print('\\nReviews description after removing punctuation and stopwrods---\\n')\n",
        "print(reviews['Description_Cleaned'][5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "771E8iJucLqU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3DrP5bjqcSUi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JIbBRc0hmjt2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_summary(text_without_removing_dot, cleaned_text):\n",
        "    sample_text = text_without_removing_dot\n",
        "    doc = nlp(sample_text)\n",
        "    sentence_list=[]\n",
        "    for idx, sentence in enumerate(doc.sents): # we are using spacy for sentence tokenization\n",
        "        sentence_list.append(re.sub(r'[^\\w\\s]','',str(sentence)))\n",
        "        \n",
        "\n",
        "#     stopwords = nltk.corpus.stopwords.words('english')\n",
        "    stopword = list(STOP_WORDS)\n",
        "    \n",
        "    tokens = parser(cleaned_text)\n",
        "    tokens = [token.orth_ for token in tokens if not token.orth_.isspace()]\n",
        "    \n",
        "    word_frequencies = {}  \n",
        "#     for word in nltk.word_tokenize(cleaned_text):\n",
        "    for word in tokens:\n",
        "        if word not in stopwords:\n",
        "            if word not in word_frequencies.keys():\n",
        "                word_frequencies[word] = 1\n",
        "            else:\n",
        "                word_frequencies[word] += 1\n",
        "\n",
        "\n",
        "    maximum_frequncy = max(word_frequencies.values())\n",
        "    print('max----------------',word_frequencies)\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
        "\n",
        "    print('sentence_list----',sentence_list)\n",
        "    sentence_scores = {}  \n",
        "    for sent in sentence_list:  \n",
        "        for word in nltk.word_tokenize(sent.lower()):\n",
        "            if word in word_frequencies.keys():\n",
        "                if len(sent.split(' ')) < 30:\n",
        "                    if sent not in sentence_scores.keys():\n",
        "                        sentence_scores[sent] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[sent] += word_frequencies[word]\n",
        "\n",
        "\n",
        "    summary_sentences = heapq.nlargest(4, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    print(\"Original Text:\\n\")\n",
        "    print(text_without_removing_dot)\n",
        "    print('\\n\\nSummarized text:\\n')\n",
        "    print(summary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnIb5fNlEjOS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import heapq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UjEJoM4EELLf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews['description_Cleaned_1']=reviews['Description_Cleaned'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xySB5272rBCi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_summary(reviews['description'][5], reviews['Description_Cleaned'][5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v9jOne94r36m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Second Approach  for text summeration__**"
      ]
    },
    {
      "metadata": {
        "id": "NuSts_vGIOGs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "1) Associate words with their grammatical counterparts. (e.g. \"city\" and \"cities\")\n",
        "\n",
        "2) Calculate the occurrence of each word in the text.\n",
        "\n",
        "3) Assign each word with points depending on their popularity.\n",
        "\n",
        "4) Detect which periods represent the end of a sentence. (e.g \"Mr.\" does not).\n",
        "\n",
        "5) Split up the text into individual sentences.\n",
        "\n",
        "6) Rank sentences by the sum of their words' points.\n",
        "\n",
        "7) Return X of the most highly ranked sentences in chronological order.\n",
        "'''"
      ]
    },
    {
      "metadata": {
        "id": "ocCQqxGtIyMr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import sys\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DjECPqOgtR0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gFw40QmrGWK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "#     args = sys.argv\n",
        "\n",
        "#     # Document name should be provided\n",
        "#     if len(args) < 2:\n",
        "#         print (\"Provide document\")\n",
        "#         exit(1)\n",
        "\n",
        "#     filename = args[1]\n",
        "    \n",
        "#     sentence_count = 3\n",
        "#     # Read number of sentences to output if provided\n",
        "#     if len(args) == 3:\n",
        "#         sentence_count = int(args[2])\n",
        "\n",
        "    # Read file as utf-8\n",
        "    document_file = codecs.open(filename, encoding='utf-8')\n",
        "    contents = document_file.read()\n",
        "\n",
        "    # Process file contents\n",
        "    nlp = spacy.load('en')\n",
        "    doc = nlp(contents)\n",
        "\n",
        "    # 1, 2, 3\n",
        "    occurrences = {}\n",
        "    def fill_occurrences(word):\n",
        "        word_lemma = lemma(word)\n",
        "        count = occurrences.get(word_lemma, 0)\n",
        "        count += 1\n",
        "        occurrences[word_lemma] = count\n",
        "\n",
        "    each_word(doc, fill_occurrences)\n",
        "\n",
        "    # 4, 5, 6\n",
        "    ranked = get_ranked(doc.sents, sentence_count, occurrences)\n",
        "\n",
        "    # 7\n",
        "    print ( \" \".join([x['sentence'].text for x in ranked]))\n",
        "\n",
        "def each_word(words, func):\n",
        "    for word in words:\n",
        "        if word.pos_ is \"PUNCT\":\n",
        "            continue\n",
        "\n",
        "        func(word)\n",
        "\n",
        "def get_ranked(sentences, sentence_count, occurrences):\n",
        "    # Maintain ranked sentences for easy output\n",
        "    ranked = []\n",
        "\n",
        "    # Maintain the lowest score for easy removal\n",
        "    lowest_score = -1\n",
        "    lowest = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        # Fill ranked if not at capacity\n",
        "        if len(ranked) < sentence_count:\n",
        "            score = get_score(occurrences, sent)\n",
        "\n",
        "            # Maintain lowest score\n",
        "            if score < lowest_score or lowest_score is -1:\n",
        "                lowest = len(ranked) + 1\n",
        "                lowest_score = score\n",
        "\n",
        "            ranked.append({'sentence': sent, 'score': score})\n",
        "            continue\n",
        "\n",
        "        score = get_score(occurrences, sent)\n",
        "        # Insert if score is greater\n",
        "        if score > lowest_score:\n",
        "            # Maintain chronological order\n",
        "            for i in xrange(lowest, len(ranked) - 1):\n",
        "                ranked[i] = ranked[i+1]\n",
        "\n",
        "            ranked[len(ranked) - 1] = {'sentence': sent, 'score': score}\n",
        "\n",
        "            # Reset lowest_score\n",
        "            lowest_score = ranked[0]['score']\n",
        "            lowest = 0\n",
        "            for i in xrange(0, len(ranked)):\n",
        "                if ranked[i]['score'] < lowest_score:\n",
        "                    lowest = i\n",
        "                    lowest_score = ranked[i]['score']\n",
        "\n",
        "    return ranked\n",
        "\n",
        "def lemma(word):\n",
        "    return word.lemma_\n",
        "\n",
        "def get_score(occurrences, sentence):\n",
        "    class Totaler:\n",
        "        def __init__(self):\n",
        "            self.score = 0\n",
        "        def __call__(self, word):\n",
        "            self.score += occurrences.get(lemma(word), 0)\n",
        "        def total(self):\n",
        "            # Should the score be divided by total words?\n",
        "            return self.score\n",
        "\n",
        "    totaler = Totaler()\n",
        "\n",
        "    each_word(sentence, totaler)\n",
        "\n",
        "    return totaler.total()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKuR2jEEI5ZU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3pQ6IB2lwkR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **NLTK AND SPACY TOKINIZER WITH EXAMPLE**"
      ]
    },
    {
      "metadata": {
        "id": "M1nvnnQ-kxe2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**nltk**"
      ]
    },
    {
      "metadata": {
        "id": "TmMUj7sGkzSU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text1 = \"It's true that the chicken was the best bamboozler in the known multiverse.\"\n",
        "tokens = word_tokenize(text1)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wZrEog0zk0M9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6xaSJJe2k3SY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**spacy**"
      ]
    },
    {
      "metadata": {
        "id": "4ve3RlHsk4n1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "print(parser)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kqoo_1yFk5bA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text1 = \"It's true that the chicken was the best bamboozler in the known multiverse.\"\n",
        "tokens = parser(text1)\n",
        "tokens = [token.orth_ for token in tokens if not token.orth_.isspace()]\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1EiDUVQlTb2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "textu = \"I'm Mr. O'Malley, and I love things, i.e., tacos etc.\"\n",
        "tokens = parser(textu)\n",
        "tokens = [token.orth_ for token in tokens if not token.orth_.isspace()]\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "faMN2y2jlsoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cUww0eFq9sXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Based on the word embeddings, spaCy offers a similarity interface for all of itâ€™s building blocks: Token, Span, Doc and Lexeme\n",
        "Chunking\n",
        "spaCy automatically detects noun-phrases as well:**\n",
        "\n",
        "chunker also computes the root of the phrase, the main word of the phrase."
      ]
    },
    {
      "metadata": {
        "id": "fyEj05zO9tEY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Ram eat a mongo and Raju does not eat a mango\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.label_, chunk.root.text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pc7pha8b90K_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# nlp = spacy.load('en_core_web_lg')\n",
        "print( nlp.vocab['banana'].vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlSvJpOdB_zd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**like sentiment **"
      ]
    },
    {
      "metadata": {
        "id": "UhpNhIMVCFUp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Jb_LQ4TCJlr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ez1vffVX_aoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "def polarity_scores(doc):\n",
        "    return sentiment_analyzer.polarity_scores(doc.text)\n",
        " \n",
        "Doc.set_extension('polarity_scores', getter=polarity_scores,force=True)\n",
        " \n",
        "# nlp = spacy.load('en')\n",
        "doc = nlp(\"The phoneâ€™s design is not good Iâ€™ve seen so far, but the battery can definitely use some improvements\")\n",
        "print(doc._.polarity_scores)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0QYDdL9aDL_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('vader_lexicon')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-mo8ScCO8gX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ":**baigram and n-gram**\n",
        "One new important addition is using bigrams. Bigrams are pairs of consecutive words. In general N-grams are tuples of N consecutive words. Hereâ€™s what I mean"
      ]
    },
    {
      "metadata": {
        "id": "3_50vSiVO8Hg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk import bigrams\n",
        "print ( list(bigrams(\" One new important addition is using bigrams\".split())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "krHpqA_hRopq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8VCPJ9iUH_c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Another Way of Sentiment using ML**"
      ]
    },
    {
      "metadata": {
        "id": "YCRnx8AoRohs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd       \n",
        "data = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
        " \n",
        "# 25000 movie reviews\n",
        "print ( data.shape )# (25000, 3) \n",
        "print ( data[\"review\"][0]  )       # Check out the review\n",
        "print ( data[\"sentiment\"][0] )         # Check out the sentiment (0/1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcTR5HZkc-ez",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print( data[\"sentiment\"][2])\n",
        "print(data[\"review\"][2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A3jnTcGxRrw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        " \n",
        "sentiment_data = list( zip(data[\"review\"].values.astype('U'), data[\"sentiment\"].values.astype('U')))\n",
        "random.shuffle(sentiment_data)\n",
        " \n",
        "# 80% for training\n",
        "train_X, train_y = zip(*sentiment_data[:20000])\n",
        " \n",
        "# Keep 20% for testing\n",
        "test_X, test_y = zip(*sentiment_data[20000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uzmDs3q5Wfsk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test_X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1mFadzCSK_P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.base import TransformerMixin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OLVp8dGhSL09",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unigram_bigram_clf = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer=\"word\",\n",
        "                                   ngram_range=(1, 2),\n",
        "                                   tokenizer=word_tokenize,\n",
        "                                   # tokenizer=lambda text: mark_negation(word_tokenize(text)),\n",
        "                                   preprocessor=lambda text: text.replace(\"<br />\", \" \"),)),\n",
        "    ('classifier', LinearSVC())\n",
        "])\n",
        " \n",
        "unigram_bigram_clf.fit(train_X, train_y)\n",
        "unigram_bigram_clf.score(test_X, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7J7s1qa3a7NE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Save Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmXYxnTOSP6V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample_prediction = unigram_bigram_clf.predict(test_X)\n",
        "\n",
        "print(\"Accuracy:\",unigram_bigram_clf.score( test_X, test_y))\n",
        "print(\"Acuuracy:\" , unigram_bigram_clf.score(test_X,sample_prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-_CPFTlkawUf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unigram_bigram_clf.predict([\"sudasrshan is not good boy\"])\n",
        "\n",
        "# pipe.predict(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKiTquwTE-yi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XL3rU6d7fzAR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **textacy**"
      ]
    },
    {
      "metadata": {
        "id": "dy0PAAj-u2Lx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[textacy](https://chartbeat-labs.github.io/textacy/getting_started/quickstart.html)"
      ]
    },
    {
      "metadata": {
        "id": "U6zjJWLzFx7P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import textacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oBVpYQjgf5lq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install textacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbygAfEGgDF6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = '''Spacy does not comes with an easily usable function for sentiment analysis,Spacy is useful in NLP ,@#$'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s973lWFtuIaB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text ='''Brutal cold that can cause frostbite within a few minutes blasted the Midwest for a second day and blew into the Northeast on Thursday, snarling air traffic and delaying Amazon and other package deliveries. Amazon said it closed some buildings, including fulfillment centers across the Midwest that were affected by the numbing low temperatures. \"We work hard to deliver on our fast, free shipping promise, but weather conditions are out of our control,\" Amazon said in a statement. \"Customer service is available to work with any customer who is experiencing an issue.\" United Parcel Service said it had suspended package deliveries and pickups in parts of upstate New York and across a wide swath of the Midwest for safety reasons as temperatures remained low and federal forecasters warned of \"dangerously low wind chill values.\" UPS listed affected areas by ZIP codes in the Midwest and New York on its website. Rival FedEx closed some of its offices early, including in Chicago and Detroit, scaled back service in cities across the Midwest, saying: \"Our priority is always safety and providing service to the best of our ability.\" The U.S. Postal Service suspended deliveries in some cities in the Midwest on Thursday \"to ensure the safety and well-being of our employees.\" The stinging cold hobbled Chicago\\'s main airports on Thursday, as it was too cold for ground workers to access and service many aircraft. Airlines canceled more than 1,400 flights on Thursday at Chicago\\'s O\\'Hare International Airport, more than half of the daily schedule at the United Airlines and American Airlines hub. More than 40 percent of the schedule, or 237 flights at Chicago Midway International Airport, were canceled, according to flight-tracking site FlightAware. Hundreds of other flights were canceled in Chicago on Wednesday due to the cold. tweet Temperatures at O\\'Hare at 5:30 a.m. were minus 21 degrees, with wind chills of 37 degrees below zero, the National Weather Service said. Delta Air Lines, American, United, Spirit Airlines and JetBlue Airways said they would waive date-change fees for travelers affected by the severe weather. Southwest Airlines, which doesn\\'t have date-change fees, said it wouldn\\'t charge travelers booked in and out of more than two dozen U.S. airports the fare difference to fly at a later date due to the extreme weather. Southwest canceled 410 flights on Thursday, about 10 percent of its schedule. While aircraft can take generally take off in low temperatures, the bitter cold limits how long ground workers can remain loading baggage or provide fuel and other essential services for aircraft. Delta, which operates hubs in Minneapolis and Detroit, prepared for the frigid conditions by increasing staffing of ground workers. Employees can take more frequent breaks indoors, as the airline can prepare for the low temperatures by moving some aircraft into heated hangars overnight, said spokesman Michael Thomas.'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ojmr3BTMggLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(textacy.text_utils.KWIC(text, 'sentiment', window_width=35))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cwgrZmMOgrtn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(textacy.preprocess_text(text, lowercase=True, no_punct=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BTYEOELzhMwi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dir(textacy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gKi0cESiBXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = textacy.Doc(text,lang='en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yHH85_Eil8Hb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lKLbsxj7kNpl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install cld2-cffi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmxhZIQXkWuE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " metadata = {\n",
        "    'title': 'Natural-language processing',\n",
        "    'url': 'https://en.wikipedia.org/wiki/Natural-language_processing',\n",
        "    'source': 'wikipedia',\n",
        " }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QQaQryzzofXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc = textacy.Doc(text,metadata=metadata,lang='en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BFAIAnGQotvV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc.metadata['url']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C7SC0vMiqonp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Analyze the Doc ( Remove stop,punctuation,number from Doc)**"
      ]
    },
    {
      "metadata": {
        "id": "hgxzXyiqo2a2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " list(textacy.extract.ngrams(doc, 2, filter_stops=True, filter_punct=True, filter_nums=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OK6JpdiBrDO5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmAAp2dkrEVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Key_Term_weight_of_word**"
      ]
    },
    {
      "metadata": {
        "id": "x9HnON1fphG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import textacy.keyterms  # note the import\n",
        "textacy.keyterms.textrank(doc, normalize='lemma', n_keyterms=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3OPwfgihrLOt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " textacy.keyterms.sgrank(doc, ngrams=(1, 2, 3, 4), normalize='lower', n_keyterms=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oymKCTELtAE5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ts = textacy.TextStats(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilkqGqMVvBPU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ts.basic_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YK37e2EqvDSC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ts.readability_stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tApV0IcfvZD9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bot = doc.to_bag_of_terms(ngrams=(1, 2, 3), named_entities=True, weighting='count',as_strings=True)\n",
        "sorted(bot.items(), key=lambda x: x[1], reverse=True)[:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VA2jEyxFw0HV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "texts = textacy.io.read_text('po.txt', lines=True)\n",
        "for text in texts:\n",
        "  doc = textacy.Doc(text,lang='en_core_web_sm')\n",
        "  print(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "evwS8NZJAxQu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**make a corpus**"
      ]
    },
    {
      "metadata": {
        "id": "SejC2GbiBCUe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ">>> import textacy.datasets  # note the import\n",
        ">>> cw = textacy.datasets.CapitolWords()\n",
        ">>> cw.download()\n",
        ">>> records = cw.records(speaker_name={'Hillary Clinton', 'Barack Obama'})\n",
        ">>> text_stream, metadata_stream = textacy.io.split_records(records, 'text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoF1FZNQ6mWc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDpzW35PA1VG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FU6KCjPJC1OO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}